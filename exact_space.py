# -*- coding: utf-8 -*-
"""exact_space

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12mZZhNAOOtHXaXjD5CD3FfooSVy_HjUK
"""

# TASK 1: Data Preparation & Exploratory Analysis

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings("ignore")

# Ensure output directories
os.makedirs("outputs", exist_ok=True)
os.makedirs("plots", exist_ok=True)

# Load dataset
df = pd.read_excel(
    "/content/data.xlsx",
    parse_dates=["time"],
    index_col="time"
)
df = df.sort_index()

# Force all non-timestamp columns into numeric
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors="coerce")

print("Data loaded:", df.shape)

# Enforce 5-min frequency
full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq="5T")
df = df.reindex(full_index)

# Handle missing values
# Interpolate short gaps (up to 1h = 12 steps), rest ffill/bfill
df = df.interpolate(limit=12)
df = df.fillna(method="bfill").fillna(method="ffill")

# Outlier treatment
z_scores = (df - df.mean()) / df.std()
df = df.mask(np.abs(z_scores) > 5)   # mask extreme values beyond z=5
df = df.interpolate().fillna(method="bfill").fillna(method="ffill")

# Summary statistics
summary_stats = df.describe(percentiles=[.05, .25, .5, .75, .95]).T
summary_stats.to_csv("outputs/summary_stats.csv")
print("Summary stats saved → outputs/summary_stats.csv")

# Correlation Matrix (robust)
df_numeric = df.select_dtypes(include=[np.number])

if df_numeric.empty:
    print("⚠️ No numeric data available after conversion.")
else:
    corr = df_numeric.corr(method="pearson", min_periods=30)  # require at least 30 valid data points
    if corr.empty:
        print("⚠️ Correlation matrix is empty: no overlapping data for correlations.")
    else:
        plt.figure(figsize=(8,6))
        sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", square=True)
        plt.title("Correlation Matrix (Numeric Only)")
        plt.savefig("plots/correlation_matrix.png", dpi=150)
        plt.close()
        print("Correlation matrix saved → plots/correlation_matrix.png")

# Visualize one week & one year
def plot_slice(start, end, name):
    subset = df.loc[start:end]
    if subset.empty:
        print(f"⚠️ No data available for slice {name}")
        return
    subset.plot(subplots=True, figsize=(12,8), title=f"{name.capitalize()} Slice")
    plt.tight_layout()
    plt.savefig(f"plots/timeseries_{name}.png", dpi=150)
    plt.close()
    print(f"Saved → plots/timeseries_{name}.png")

plot_slice("2020-01-01","2020-01-07","week")
plot_slice("2020-01-01","2020-12-31","year")

# Data Quality Report
report = pd.DataFrame({
    "dtype": df.dtypes,
    "% missing": df.isna().mean().round(3),
    "min": df.min(),
    "max": df.max(),
    "mean": df.mean()
})
report.to_csv("outputs/data_quality_report.csv")
print("Data quality report saved → outputs/data_quality_report.csv")

#2nd task in 1st
# Define shutdown/idle rule
# Example: shutdown if temperature < 100°C and inlet draft < 5
# (tune thresholds based on process domain knowledge)
df["is_shutdown"] = ((df["Cyclone_Inlet_Gas_Temp"] < 100) &
                     (df["Cyclone_Inlet_Draft"] < 5)).astype(int)

# Detect shutdown events (start, end, duration)
shutdown_periods = []
is_down = False
start_time = None

for t, val in df["is_shutdown"].items():
    if val == 1 and not is_down:
        # entering shutdown
        start_time = t
        is_down = True
    elif val == 0 and is_down:
        # exiting shutdown
        end_time = t
        duration = (end_time - start_time).total_seconds()/60  # in minutes
        shutdown_periods.append([start_time, end_time, duration])
        is_down = False

# Handle case where shutdown continues till last timestamp
if is_down and start_time is not None:
    end_time = df.index[-1]
    duration = (end_time - start_time).total_seconds()/60
    shutdown_periods.append([start_time, end_time, duration])

# Convert to DataFrame
shutdown_df = pd.DataFrame(shutdown_periods,
                           columns=["start", "end", "duration_minutes"])

# Compute totals
total_shutdowns = len(shutdown_df)
total_downtime_hours = shutdown_df["duration_minutes"].sum()/60

print(f"Detected {total_shutdowns} shutdown events")
print(f"Total downtime: {total_downtime_hours:.1f} hours")

# Save results
shutdown_df.to_csv("outputs/shutdown_periods.csv", index=False)
print("Shutdown periods saved → outputs/shutdown_periods.csv")

# Visualization – highlight shutdowns in 1 year
year_start = "2020-01-01"
year_end   = "2020-12-31"
subset = df.loc[year_start:year_end]

plt.figure(figsize=(15,5))
plt.plot(subset.index, subset["Cyclone_Inlet_Gas_Temp"], label="Inlet Temp (°C)", color="steelblue")

# Highlight shutdowns
for _, row in shutdown_df.iterrows():
    if row["start"] >= pd.to_datetime(year_start) and row["start"] <= pd.to_datetime(year_end):
        plt.axvspan(row["start"], row["end"], color="red", alpha=0.3)

plt.title("Cyclone Inlet Gas Temperature with Shutdown Periods Highlighted (2020)")
plt.xlabel("Time")
plt.ylabel("Inlet Temp (°C)")
plt.legend()
plt.tight_layout()
plt.savefig("plots/shutdowns_year.png", dpi=150)
plt.close()
print("Shutdown visualization saved → plots/shutdowns_year.png")

""":# TASK 3: Machine State Segmentation (Clustering)

"""

#3rd task
# TASK 3: Machine State Segmentation (Clustering)

from sklearn.cluster import KMeans

# Exclude shutdown data
active_df = df[df["is_shutdown"] == 0].copy()

# Feature Engineering
# Raw features
features = active_df[[
    "Cyclone_Inlet_Gas_Temp",
    "Cyclone_Gas_Outlet_Temp",
    "Cyclone_Outlet_Gas_draft",
    "Cyclone_cone_draft",
    "Cyclone_Inlet_Draft",
    "Cyclone_Material_Temp"
]].copy()

# Add rolling mean & std (smooth operating conditions)
rolling_window = 6  # 30 minutes (6 * 5min)
for col in features.columns:
    features[f"{col}_roll_mean"] = features[col].rolling(rolling_window, min_periods=1).mean()
    features[f"{col}_roll_std"]  = features[col].rolling(rolling_window, min_periods=1).std()

# Add first differences (detecting sudden changes)
for col in [
    "Cyclone_Inlet_Gas_Temp",
    "Cyclone_Gas_Outlet_Temp",
]:
    features[f"{col}_delta"] = features[col].diff().fillna(0)

# Drop any residual NaN
features = features.dropna()

# Normalize Features
features_norm = (features - features.mean()) / features.std()

# Apply Clustering (KMeans Example)
k = 4  # number of states (adjust using elbow method / silhouette score)
kmeans = KMeans(n_clusters=k, random_state=42)
active_df = active_df.loc[features_norm.index].copy()   # align indices
active_df["cluster"] = kmeans.fit_predict(features_norm)

print("Clustering complete. Detected states:", active_df["cluster"].unique())

# Cluster Summary Statistics
summary_list = []
for c in sorted(active_df["cluster"].unique()):
    cluster_data = active_df[active_df["cluster"]==c]
    stats = cluster_data[[
        "Cyclone_Inlet_Gas_Temp",
        "Cyclone_Gas_Outlet_Temp",
        "Cyclone_Outlet_Gas_draft",
        "Cyclone_cone_draft",
        "Cyclone_Inlet_Draft",
        "Cyclone_Material_Temp"
    ]].agg(['mean','std','min','max','median','quantile'])
    summary_list.append({
        "cluster": c,
        "n_samples": len(cluster_data),
        "mean_inlet_temp": cluster_data["Cyclone_Inlet_Gas_Temp"].mean(),
        "mean_outlet_temp": cluster_data["Cyclone_Gas_Outlet_Temp"].mean(),
        "mean_inlet_draft": cluster_data["Cyclone_Inlet_Draft"].mean()
    })

clusters_summary = pd.DataFrame(summary_list)

# Frequency and duration statistics
active_df["state_change"] = (active_df["cluster"] != active_df["cluster"].shift(1)).astype(int)
active_df["episode_id"] = active_df["state_change"].cumsum()

episode_stats = active_df.groupby(["cluster","episode_id"]).size().reset_index(name="duration_steps")
episode_stats["duration_minutes"] = episode_stats["duration_steps"] * 5
freq_duration_summary = episode_stats.groupby("cluster")["duration_minutes"].describe()

# Save results
clusters_summary.to_csv("outputs/clusters_summary.csv", index=False)
freq_duration_summary.to_csv("outputs/clusters_duration_summary.csv")

print("Clusters summary saved → outputs/clusters_summary.csv")
print("Duration summary saved → outputs/clusters_duration_summary.csv")

# Visualization – One Week with States Highlighted
week_data = active_df["2020-01-01":"2020-01-07"]

plt.figure(figsize=(14,5))
plt.plot(week_data.index, week_data["Cyclone_Inlet_Gas_Temp"], label="Inlet Temp", color="steelblue")
plt.scatter(week_data.index, week_data["Cyclone_Inlet_Gas_Temp"],
            c=week_data["cluster"], cmap="tab10", s=10, alpha=0.8, label="Cluster State")
plt.title("Clusters/Operational States in Sample Week")
plt.xlabel("Time")
plt.ylabel("Inlet Temp (°C)")
plt.legend()
plt.savefig("plots/clustered_week.png", dpi=150)
plt.close()
print("Cluster visualization saved → plots/clustered_week.png")

"""\# TASK 4: Contextual Anomaly Detection + Root Cause

"""

# TASK 4: Contextual Anomaly Detection + Root Cause

from sklearn.ensemble import IsolationForest

# Prepare features for anomaly detection
features_norm = (features - features.mean()) / features.std()
features_norm = features_norm.loc[active_df.index]  # align with clustered df

# Detect anomalies per cluster
anomalies = []

for c in sorted(active_df["cluster"].unique()):
    cluster_idx = active_df["cluster"] == c
    cluster_data = features_norm[cluster_idx]
    if cluster_data.shape[0] < 50:
        continue  # too few points to train isoForest

    iso = IsolationForest(contamination=0.01, random_state=42)
    scores = iso.fit_predict(cluster_data)
    anomaly_flags = pd.Series(scores, index=cluster_data.index)
    anomaly_flags = (anomaly_flags == -1).astype(int)

    # store anomaly flags
    active_df.loc[cluster_idx, "anomaly"] = anomaly_flags

    # capture anomalies in list
    for t, is_anom in anomaly_flags.items():
        if is_anom == 1:
            anomalies.append([t, c])

# Consolidated anomalies DataFrame
anomalies_df = pd.DataFrame(anomalies, columns=["timestamp","cluster"]).sort_values("timestamp")
anomalies_df.to_csv("outputs/anomalous_periods.csv", index=False)

print(f"Detected {len(anomalies_df)} anomalies across all clusters")
print("Saved → outputs/anomalous_periods.csv")

# Segment anomalies into events (start-end-duration)
events = []
if not anomalies_df.empty:
    grouped = (anomalies_df["timestamp"].sort_values().diff() > pd.Timedelta("15min")).cumsum()
    anomalies_df["event_id"] = grouped
    for eid, group in anomalies_df.groupby("event_id"):
        start = group["timestamp"].min()
        end   = group["timestamp"].max()
        duration = (end - start).total_seconds()/60
        clusters_involved = group["cluster"].unique().tolist()
        events.append([eid, start, end, duration, clusters_involved])

events_df = pd.DataFrame(events, columns=["event_id","start","end","duration_minutes","clusters"])
events_df.to_csv("outputs/anomalous_events_summary.csv", index=False)
print("Saved event summary → outputs/anomalous_events_summary.csv")

# Visualize Selected Anomalies
# Pick 3-5 interesting events (longest or random samples)
if not events_df.empty:
    sample_events = events_df.sort_values("duration_minutes", ascending=False).head(3)

    for _, row in sample_events.iterrows():
        start, end = row["start"], row["end"]
        window = active_df.loc[start - pd.Timedelta("1H"): end + pd.Timedelta("1H")].copy()

        plt.figure(figsize=(12,6))
        plt.plot(window.index, window["Cyclone_Inlet_Gas_Temp"], label="Inlet Temp", color="steelblue")
        plt.plot(window.index, window["Cyclone_Gas_Outlet_Temp"], label="Outlet Temp", color="darkorange")
        plt.plot(window.index, window["Cyclone_Inlet_Draft"], label="Inlet Draft", color="green")

        # mark anomaly interval
        plt.axvspan(start, end, color="red", alpha=0.2, label="Anomaly Event")

        # mark cluster states (color-coded scatter)
        plt.scatter(window.index, window["Cyclone_Inlet_Gas_Temp"],
                    c=window["cluster"], cmap="tab10", s=12, alpha=0.7)

        plt.title(f"Anomaly Event {row['event_id']} (duration {row['duration_minutes']:.1f} min)")
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"plots/anomaly_event_{row['event_id']}.png", dpi=150)
        plt.close()
        print(f"Plot saved → plots/anomaly_event_{row['event_id']}.png")

# TASK 5: Short-Horizon Forecasting (1 hour = 12 steps)
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.ensemble import RandomForestRegressor

# Prepare Target Variable
target = df["Cyclone_Inlet_Gas_Temp"].dropna()

# Train/Test Split -> last 3 months for test
split_date = target.index[-(12*24*90)]  # ~ last 90 days
train, test = target[:split_date], target[split_date:]

print("Train size:", train.shape, "Test size:", test.shape)

# Baseline Forecast (Persistence)
# Forecast = last observed value carried forward
baseline_pred = test.shift(1).fillna(method="bfill")

# ARIMA Model
# Simple ARIMA(2,1,2) as example
try:
    arima = ARIMA(train, order=(2,1,2))
    arima_fit = arima.fit()
    arima_forecast = arima_fit.forecast(steps=len(test))
except Exception as e:
    print("ARIMA failed:", e)
    arima_forecast = pd.Series(index=test.index, data=np.nan)

# RandomForest with Lag Features
def make_lagged_features(series, lags=12):
    df_lag = pd.DataFrame({f"lag_{i}": series.shift(i) for i in range(1,lags+1)})
    df_lag["y"] = series.values
    return df_lag.dropna()

lagged = make_lagged_features(target, lags=12)  # 1 hr worth of lags
train_lag, test_lag = lagged.loc[:split_date], lagged.loc[split_date:]
X_train, y_train = train_lag.drop("y", axis=1), train_lag["y"]
X_test, y_test   = test_lag.drop("y", axis=1), test_lag["y"]

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_pred = pd.Series(rf.predict(X_test), index=y_test.index)

# Evaluate Models
def evaluate(y_true, y_pred, name):
    rmse = sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    print(f"{name:12s} → RMSE={rmse:.2f}, MAE={mae:.2f}")
    return rmse, mae

results_eval = {}
results_eval["baseline"] = evaluate(test, baseline_pred, "Baseline")
results_eval["arima"] = evaluate(test, arima_forecast, "ARIMA")
results_eval["randomforest"] = evaluate(y_test, rf_pred, "RandomForest")

# Save Forecast Results
results = pd.DataFrame({
    "timestamp": test.index[-len(y_test):],
    "true": y_test.values,
    "baseline": baseline_pred[-len(y_test):].values,
    "arima": arima_forecast[-len(y_test):].values,
    "randomforest": rf_pred.values,
})
results.to_csv("outputs/forecasts.csv", index=False)
print("Forecasts saved → outputs/forecasts.csv")

# Visualization
plt.figure(figsize=(12,6))
plt.plot(results["timestamp"], results["true"], label="True", color="black")
plt.plot(results["timestamp"], results["baseline"], label="Baseline", alpha=0.8)
plt.plot(results["timestamp"], results["arima"], label="ARIMA", alpha=0.8)
plt.plot(results["timestamp"], results["randomforest"], label="RandomForest", alpha=0.8)
plt.title("Cyclone_Inlet_Gas_Temp – Forecast Comparison (1hr horizon)")
plt.legend()
plt.tight_layout()
plt.savefig("plots/forecast_comparison.png", dpi=150)
plt.close()
print("Saved plot → plots/forecast_comparison.png")